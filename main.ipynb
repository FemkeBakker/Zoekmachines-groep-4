{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoekmachines Project 2022\n",
    "\n",
    "This notebook contains a pipeline which you can use and extend to return relevant passages given users' queries. The pipeline contains following parts:\n",
    "* Dataloader\n",
    "* Preprocessing\n",
    "* Full-ranking + Feature construction\n",
    "* Re-ranking\n",
    "* Evaluation\n",
    "* Submission to CodaLab leaderboard  \n",
    "  \n",
    "Amongst them, dataloader, preprocessing, full-ranking + feature construction, re-ranking and evaluaton are modules of an information retrieval system.\n",
    "The implementation for them are very basic, and so there is large room for you to improve them.\n",
    "\n",
    "At the end of the pipeline you are asked to submit your top 100 ranked list of passages on the test set to the CodaLab leaderboard. This way you can keep track of other teams performance and measure yourself with your peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to check whether you have the right Python version and the necessary packages installed. \n",
    "Please run the cell below to ensure this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python is niet gevonden. Voer zonder argumenten uit om te installeren vanuit de Microsoft Store of schakel deze snelkoppeling uit via Instellingen > Aliassen voor app-uitvoering beheren.\n"
     ]
    }
   ],
   "source": [
    "!python3 -V # please make sure this is python 3\n",
    "# !pip3 install torch\n",
    "# !pip3 install numpy\n",
    "# !pip3 install nltk\n",
    "# !pip3 install wheel\n",
    "# !pip3 install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba453bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even kieke of ie t doet lol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports below are all you need for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader\n",
    "\n",
    "Here are information for the datasets we provided.\n",
    "* Regrading passges, we have a large passge collection `passages_large.json` and a small passage collection `passages_small.json`. Note that `passages_large.json` are more larger and so contains more labelled passges. Therefore, the retrieval on `passages_large.json` can potentially lead better performance than on `passages_small.json`. We encourage you to use the larger one.\n",
    "* Regrading queries, we have a training set `training_queries.json` with 8000 queries, a validation set `validation_queries.json` with 200 queries, and a test set `test_queries.json` with 200 queries.\n",
    "* Regarding labels, we have corresponding label sets `training_labels.json`, `validation_labels.json` and `test_labels.json` for queries on training, validation and test sets, respectively, where each query id has one or multiple corresponding labelled passage id(s) as well as the relevance score(s) of labelled passage(s) to the query. \n",
    "** The relevance scores on the training set are not graded (i.e., the scores are always 1), while the scores on the validation and test sets are graded (i.e., the scores can be 1, 2, 3, etc.). The larger the score, the more relevant the passge is. \n",
    "** The label set on the test set `Test_labels.json` is unseen, and the evaluation on the CodaLab leaderboard is conducted on this unseen test set.\n",
    "\n",
    "Here are more details.\n",
    "\n",
    "|Dataset Name |The number of records |Format|\n",
    "|:---|:---|:---|\n",
    "|passages_large.json  | 10,000,000  | {passge_id: passage_text, passge_id: …}|\n",
    "|passages_small.json  |1,000,000    |  {passge_id: passage_text, passge_id: …}|\n",
    "|training_queries.json | 8,000       |  {query_id: query_text, query_id: …}|\n",
    "|validation_queries.json | 200      |{query_id: query_text, query_id: …}\n",
    "|test_queries.json        | 200      |{query_id:, query_text, query_id: …}|\n",
    "|training_labels.json      | 8,000      |{query_id:  {passage_id: relevance_score, passage_id: …}, query_id: …}|\n",
    "|validation_labels.json    |200   |{query_id:  {passage_id: relevance_score, passage_id: …}, query_id: …}|\n",
    "|test_labels.json (unseen)          |200     | {query_id:  {passage_id: relevance_score, passage_id: …}, query_id: …}|\n",
    "\n",
    "\n",
    "To load the data, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load passages from: data/passages_small.json\n",
      "peak memory: 1037.60 MiB, increment: 849.95 MiB\n",
      "load queries from: data/training_queries.json\n",
      "load queries from: data/training_queries.json\n",
      "load queries from: data/training_queries.json\n",
      "load queries from: data/training_queries.json\n",
      "peak memory: 720.27 MiB, increment: 4.14 MiB\n",
      "load queries from: data/validation_queries.json\n",
      "load queries from: data/validation_queries.json\n",
      "load queries from: data/validation_queries.json\n",
      "load queries from: data/validation_queries.json\n",
      "peak memory: 720.02 MiB, increment: 0.01 MiB\n",
      "load queries from: data/test_queries.json\n",
      "load queries from: data/test_queries.json\n",
      "load queries from: data/test_queries.json\n",
      "load queries from: data/test_queries.json\n",
      "peak memory: 720.03 MiB, increment: 0.00 MiB\n",
      "Load labels from: data/training_labels.json\n",
      "Load labels from: data/training_labels.json\n",
      "Load labels from: data/training_labels.json\n",
      "Load labels from: data/training_labels.json\n",
      "peak memory: 726.06 MiB, increment: 6.03 MiB\n",
      "Load labels from: data/validation_labels.json\n",
      "Load labels from: data/validation_labels.json\n",
      "Load labels from: data/validation_labels.json\n",
      "Load labels from: data/validation_labels.json\n",
      "peak memory: 724.62 MiB, increment: 0.06 MiB\n"
     ]
    }
   ],
   "source": [
    "def passage_loader(path):\n",
    "    print(\"load passages from: {}\".format(path))   \n",
    "    passages = json.load(open(path, 'r', encoding=\"utf-8\", errors=\"ignore\"))    \n",
    "    return passages\n",
    "\n",
    "def query_loader(path):    \n",
    "    print(\"load queries from: {}\".format(path))\n",
    "    queries = json.load(open(path, 'r'))    \n",
    "    return queries\n",
    "\n",
    "\n",
    "def label_loader(path):\n",
    "    print(\"Load labels from: {}\".format(path))\n",
    "    labels = json.load(open(path, 'r'))    \n",
    "    return labels\n",
    "\n",
    "# you can choose passages_small.json or passages_large.json \n",
    "%memit passages = passage_loader(\"data/passages_small.json\")\n",
    "\n",
    "%memit queries_training = query_loader(\"data/training_queries.json\")\n",
    "%memit queries_validation = query_loader(\"data/validation_queries.json\")\n",
    "%memit queries_test = query_loader(\"data/test_queries.json\")\n",
    "\n",
    "%memit labels_training = label_loader(\"data/training_labels.json\")\n",
    "%memit labels_validation = label_loader(\"data/validation_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a loaded query, a label and a passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qid_1',\n",
       " ')what was the immediate impact of the success of the manhattan project?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(queries_training.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qid_1', {'pid_2255197': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(labels_training.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pid_811758',\n",
       " 'The file browser will appear. Select the DXF File you want to import and click Import. 3. Position Image on Canvas. Using the cursor select where you want the image to be placed and click and drag to position the image on the canvas. 4. Edit your DXF file. Make your edits to the image. 5.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(passages.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing step only has a tokenizer based on spaces.  \n",
    "Feel free to implement your own preprocessing steps, like:  \n",
    "* tokenising words with more advanced methods (e.g., nltk, spaCy and so on)\n",
    "* lowercasing\n",
    "* stemming\n",
    "* stop words removal, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 818.98 MiB, increment: -6.76 MiB\n",
      "peak memory: 482.38 MiB, increment: -340.46 MiB\n",
      "peak memory: 200.99 MiB, increment: -113.17 MiB\n",
      "peak memory: 2232.12 MiB, increment: 2024.39 MiB\n"
     ]
    }
   ],
   "source": [
    "def process_passages(passages):\n",
    "    passages_tokenised = {}\n",
    "    for passage_id in passages.keys():\n",
    "        passages_tokenised[passage_id] = passages[passage_id].split()\n",
    "    return passages_tokenised\n",
    "\n",
    "\n",
    "def process_queries(queries):   \n",
    "    queries_tokenised = {}  \n",
    "    for query_id in queries.keys():\n",
    "        queries_tokenised[query_id] = queries[query_id].split()\n",
    "    return queries_tokenised  \n",
    "\n",
    "\n",
    "%memit tokenised_queries_training = process_queries(queries_training)\n",
    "%memit tokenised_queries_validation = process_queries(queries_validation)\n",
    "%memit tokenised_queries_test = process_queries(queries_test)\n",
    "\n",
    "%memit tokenised_passages = process_passages(passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a tokenised query and a passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')what', 'was', 'the', 'immediate', 'impact', 'of', 'the', 'success', 'of', 'the', 'manhattan', 'project?']\n"
     ]
    }
   ],
   "source": [
    "print(tokenised_queries_training['qid_1']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'file', 'browser', 'will', 'appear.', 'Select', 'the', 'DXF', 'File', 'you', 'want', 'to', 'import', 'and', 'click', 'Import.', '3.', 'Position', 'Image', 'on', 'Canvas.', 'Using', 'the', 'cursor', 'select', 'where', 'you', 'want', 'the', 'image', 'to', 'be', 'placed', 'and', 'click', 'and', 'drag', 'to', 'position', 'the', 'image', 'on', 'the', 'canvas.', '4.', 'Edit', 'your', 'DXF', 'file.', 'Make', 'your', 'edits', 'to', 'the', 'image.', '5.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenised_passages['pid_811758'])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Ranking + Feature Construction\n",
    "\n",
    "Given a user query, full-ranking aims to quickly and roughly rank all passages and return a ranked list of passages.\n",
    "Here, we implement __Term Frequency (TF)__ and regard it as a full-ranking method.\n",
    "\n",
    "You are encouraged to implement more advanced full-ranking methods, such as TF-IDF, BM25 and so on. You are also encouraged to add a __relevance feedback module__ here to further improve the performance.\n",
    "You are asked to implement a full-ranking method by yourself, and you are not allowed to use some off-the-shelf pacakges, such as scikit-learn, pandas and so on.\n",
    "\n",
    "Next, let's conduct full-ranking on the __training__ set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf import TermFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1846.24 MiB, increment: -77.58 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit full_ranker = TermFrequency(tokenised_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each query, calculte scores of all passages on the training set.\n",
    "%memit scores = full_ranker.score(tokenised_queries_training)\n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the full-ranking calculation finishes, the top 100 ranked list of passages are stored. \n",
    "In parallel, the features for the next re-ranking are also constructed. \n",
    "Here we only consider two features, __TF scores__ and __passge lengths__.\n",
    "\n",
    "Before outputing the ranked list, you need first create an output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"output/\"):\n",
    "    os.makedirs(\"output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the result file and build_features  \n",
    "with codecs.open(\"output/full_ranking_training_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score[:100]:\n",
    "            ranking+=1         \n",
    "            feature_1 = score\n",
    "            feature_2 = len(full_ranker.passages[p_id])             \n",
    "    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(feature_1), str(feature_2), \"full_ranking_on_the_training_set\"])+os.linesep) \n",
    "\n",
    "print(\"Produce file {}\".format(\"output/full_ranking_training_result.text\"))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can conduct full-ranking on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query, calculte scores of all passages on the validation set.\n",
    "%memit scores = full_ranker.score(tokenised_queries_validation)\n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score\n",
    "    \n",
    "with codecs.open(\"output/full_ranking_validation_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score[:100]:\n",
    "            ranking+=1         \n",
    "            feature_1 = score\n",
    "            feature_2 = len(full_ranker.passages[p_id])             \n",
    "    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(feature_1), str(feature_2), \"full_ranking_on_the_validation_set\"])+os.linesep) \n",
    "\n",
    "print(\"Produce file {}\".format(\"output/full_ranking_validation_result.text\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can conduct full-ranking on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each query, calculte scores of all passages on the test set.\n",
    "%memit scores = full_ranker.score(tokenised_queries_test)\n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score\n",
    "    \n",
    "with codecs.open(\"output/full_ranking_test_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score[:100]:\n",
    "            ranking+=1         \n",
    "            feature_1 = score\n",
    "            feature_2 = len(full_ranker.passages[p_id])             \n",
    "    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(feature_1), str(feature_2), \"full_ranking_on_the_test_set\"])+os.linesep) \n",
    "\n",
    "print(\"Produce file {}\".format(\"output/full_ranking_test_result.text\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ranking\n",
    "\n",
    "Based on the top 100 ranked list of passages from full-ranking, re-ranking is conducted to further carefully rank the top 100 passages from full-ranking.   \n",
    "Re-ranking methods are usually based on neural networks, and more complex than full-ranking ones.\n",
    "\n",
    "Here, we implement __RankNet__ （https://dl.acm.org/doi/pdf/10.1145/1102351.1102363) and regard it as a re-ranking method.\n",
    "\n",
    "You are encouraged to implement more advanced re-ranking methods, such as LambdaRank or even embedding-based rankers.\n",
    "Also, you are asked to implement a re-ranking method by yourself, and you are not allowed to use some off-the-shelf re-ranking models.\n",
    "\n",
    "First, we need set the hyperparameters for RankNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for RankNet\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", type=int, default=30)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--input_size\", type=int, default=2)\n",
    "parser.add_argument(\"--hidden_size1\", type=int, default=128)\n",
    "parser.add_argument(\"--hidden_size2\", type=int, default=128)\n",
    "parser.add_argument(\"--output_size\", type=int, default=1)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "parser.add_argument(\"--random_seed\", type=int, default=0)\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "torch.cuda.manual_seed_all(args.random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranknet import train, inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to train RankNet on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full-ranking result on the training set.\n",
    "q_id = []\n",
    "features = []\n",
    "labels = []\n",
    "        \n",
    "print(\"Load file {}\".format(\"output/full_ranking_training_result.text\"))\n",
    "with codecs.open(\"output/full_ranking_training_result.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split('\\t')\n",
    "        q_id.append(content[0]) \n",
    "        features.append([float(content[3]),float(content[4])])\n",
    "        labels.append(labels_training[content[0]][content[1]] if content[1] in labels_training[content[0]] else 0)\n",
    "\n",
    "# train model\n",
    "%memit train(args, q_id, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Then, you need to conduct inference on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full-ranking result on the validation set.\n",
    "\n",
    "print(\"Load file {}\".format(\"output/full_ranking_validation_result.text\")) \n",
    "q_id = []\n",
    "p_id = []\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/full_ranking_validation_result.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split('\\t')\n",
    "        q_id.append(content[0]) \n",
    "        features.append([float(content[3]),float(content[4])])\n",
    "        p_id.append(content[1])\n",
    "\n",
    "# conduct inference on the validation set.\n",
    "%memit scores = inference(args, q_id, p_id, features) \n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score\n",
    "        \n",
    "with codecs.open(\"output/re_ranking_validation_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score:\n",
    "            ranking+=1           \n",
    "                    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(score), \"re_ranking_on_the_validation_set\"])+os.linesep)\n",
    "\n",
    "# output the result file. \n",
    "print(\"Produce file {}\".format(\"re_ranking_validation_result.text\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similaly, you need to conduct inference on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full-ranking result on the test set.\n",
    "\n",
    "print(\"Load file {}\".format(\"output/full_ranking_test_result.text\")) \n",
    "q_id = []\n",
    "p_id = []\n",
    "features = []\n",
    "        \n",
    "with codecs.open(\"output/full_ranking_test_result.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split('\\t')\n",
    "        q_id.append(content[0]) \n",
    "        features.append([float(content[3]),float(content[4])])\n",
    "        p_id.append(content[1])\n",
    "\n",
    "# conduct inference on the validation set.\n",
    "%memit scores = inference(args, q_id, p_id, features) \n",
    "\n",
    "# rank the calclulated scores from largest to smallest.\n",
    "for q_id, p2score in scores.items():\n",
    "    sorted_p2score=sorted(p2score.items(), key=lambda x:x[1], reverse = True)\n",
    "    scores[q_id]=sorted_p2score\n",
    "        \n",
    "with codecs.open(\"output/re_ranking_test_result.text\", \"w\", \"utf-8\") as file:\n",
    "    for q_id, p2score in scores.items():\n",
    "        ranking=0\n",
    "        for (p_id, score) in p2score:\n",
    "            ranking+=1           \n",
    "                    \n",
    "            file.write('\\t'.join([q_id, p_id, str(ranking), str(score), \"re_ranking_on_the_test_set\"])+os.linesep)\n",
    "\n",
    "# output the result file. \n",
    "print(\"Produce file {}\".format(\"re_ranking_test_result.text\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Here, we evaluate the final ranked lists on the validation with Mean Reciprocal Rank (MRR).\n",
    "We encourage you to add more metrics like NDCG that considers graded relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate full-ranking on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "with codecs.open(\"output/full_ranking_validation_result.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split('\\t')\n",
    "        scores[content[0]].append(content[1])\n",
    "\n",
    "print(\"Full-ranking\")\n",
    "print('MRR@{}: {:.4f}'.format(100, mrr(scores, labels_validation, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate re-ranking on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = defaultdict(list)\n",
    "with codecs.open(\"output/re_ranking_validation_result.text\", \"r\", \"utf-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        content = line.split('\\t')\n",
    "        scores[content[0]].append(content[1])\n",
    "\n",
    "print(\"Re-ranking\")\n",
    "print('MRR@{}: {:.4f}'.format(100, mrr(scores, labels_validation, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to CodaLab Leaderboard\n",
    "\n",
    "You are asked to zip the final ranked lists on the validation and test sets and then submit the zipped file to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip results\n",
    "studentnumber = \"201814828\"\n",
    "studentname = \"ChuanMeng\"\n",
    "\n",
    "filename = f\"{studentnumber}_{studentname}_codalab_submission.zip\"\n",
    "\n",
    "with ZipFile(\"output/\"+filename, 'w') as zipObj:\n",
    "    zipObj.write(\"output/re_ranking_validation_result.text\",\"re_ranking_validation_result.text\")\n",
    "    zipObj.write(\"output/re_ranking_test_result.text\",\"re_ranking_test_result.text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "042909f354144ee6793a364bae787966921fdf09c1d3de61057c525b627ddc4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
